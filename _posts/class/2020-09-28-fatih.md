---
title: "Econ 8192"
excerpt_separator: "<!--more-->"
toc: false
categories:
  - talk
tags:
  - machine learning
  - econ 8192
---




## 2020 October 12

Papers turn into projects.

Sometimes an interesting piece of the paper is removed because the focus needs to be elsewhere.

"Assumption possibility frontier". We can't model everything.We want the minimum set of features.

"A way to become a mature economist is to write the most appropriate model for the question."

Fatih often votes to hire someone based on the appendix of their paper.










## 2020 October 5

Firm effects and worker effects. Firms pay different price for same worker.
Fatih has paper about worker mismatch.

Inequality: 
Are workers becoming more diveregent, or are firms paying more and more different for same workers?

Lamadon and Marrissa

## Cristian's presentation

Scarring effect

- 9 log points larger for workers in industries exposed to china shock
- neglibile for low income workers
- high-income: 32 log points large than displaced workers in no-exposed industries
- if worker switches industries, effect fades out over 3 years

Fatih wrote papers trying to deconfirm, but found even larger effects.

#### Questions

What are conseq of China shock?  
effect of minimum wage, human capital, and severence payments  
what is optimal policy of severence and min wage

Fatih advice: 
Thinking about how to clearly present a paper forces you to think clearly.
Bob Lucas has very clear thinking.

#### Mechanism:

High skilled workers lose more human capital when they switch industries

Low skill workers are closer to the minimum wage so firms cannot undo severence payments by reducing wages 

(random thought: home production lower in cities increases necessity of min wage?)

(find source for claim about amazon workers pay)

David's paper - great recession - sector specific shocks  
Fatih's paper: "heterogeneity in scarring effects








## 2020 Sept 28 

Was worried it was in Hanson, but I guess I just joined a bit early.

[the workshop's wiki](https://sites.google.com/a/umn.edu/fguvenen-workshop/materials/resources) has slides and trove of info about Big Data.

The way Raj views big data is same approach Fatih likes

- How do you cut bit data so you reveal patterns that are actually useful?
- What can you do?
- What are the *wrong* things to do?

Athey and Imbens are about the fancy algorithms; entirely different approach.

A lot of what makes or breaks papers are points that look very simple.
The things that change the world are always beneath contempt.


When Fatih talks with editors at these journals, oftentimes they ask for things that just don't apply to samples of this scale.

### Method of simulated moments

In any structural model, you will use **Method of Simulated Moments**

Suppose you have a structural model and want to estimate a $K\times 1$ vector of structural parameters $\theta$.

for example, in a b-cycle model, these could be risk aversion, time discount, persistence and innovation variance of TFP shocks

(With *estimation*, you write one model and allow any value for the parameters so that the model comes as close to the data as possible.
You give the model the best chance to explain the world.
Put the model on a pedestal.
With calibration, I assign parameters in a way that is agnostic to the question I am trying to explain. Turn my back to the data.
We will be doing something in between because for some parameters, we have no evidence about their value.)

Basic Algo:

* Pick $R$ moments and compute them from the US data $(m(X^{US}))$.
* Pick a perticular value for $\theta$ and solve and simulate the structural model
* Compute the same R moments using simulated data $f(X^M, \theta)$
* Compare the model and data moments and vary $\theta$ until the distance between the two is minimized.

"Large sample standard errors?"

Wald objective fucntion: quadratic 

parts of any quantitative work that are swept under the rug












## Sept 21 2020

Philosophy: Serious Theory + Serious Empirical Work

Fatih's thesis was not mostly about a general equilibrium model. 
But he had another paper about some gmm metric. 
And this caused confusion. 
People who listened to his talk looked at him as an econometrician and held him to a different standard.

Put together a model. Solve that model carefully.
Calibration simulation.
But doesn't require you to be a serious econometrician.

Be clear about the part of your paper which is novel.
If you claim that you are establishing a new empirical relationship, that raises the bar.
Already tons of facts out there begging for explanations.

Every year, we have students who are smart and creative but crash and burn because of an empirical component.
Some person watiching will know that one of the empirical bits isn't true/

> The first duty of an economist is to describe correctly what is out there: 
a valid description without a deep explanation is worth a thousand times more 
than a clever explanation of nonexistant facts.  
--*(Samuelson 1964)*

Oof that's a feel.

You don't want to write a thesis without a member of the comittee is an expert on that topic.

COVID: Big distributionial shocks, nonlinearities, constraints.
Analogy to how Newtonian mechanics breaks down near the speed of light.
Financial crisis: most of the literature was ignoring the zero lower bound.

80s and 90s research methodology: Calibration and checking simple moments.  
2000s" dynamics, implulse responses, msm, indirect inference  
2010s: larger richer data sets, more serious comparison of model and data,   
strongest candidates now trained in panel data econometrics, now increasly big data and machine learning



