---
title: rambling about misinformation and encryption.
categories: online
tags:
  - incoherent ramble
---

[Here's the post that provoked this.](https://www.nytimes.com/2021/02/03/technology/personaltech/telegram-signal-misinformation.html?smtyp=cur&smid=tw-nytimes)

> The apps offer “end to end encryption,” which is a jargony way to describe messages that get scrambled to become indecipherable to anyone except for the sender and the recipient.  
>The obvious benefit is that people are ensured privacy. The possible downside is that it’s tougher for the companies and law enforcement to hold misinformation spreaders and criminals accountable because their messages won’t be accessible.

My gut feeling is that 
lumping in "misinformation spreaders" with criminals 
reveals a misunderstanding of the goals and methods of content moderation.

---

## Type 1 and Type 2 errors.

There's lots of times in life where we want to distinguish between two possibilities and intervene in only one of those two cases:
- Treat the diseased, don't give unecessary treatment to the healthy.
- Drill and fill a cavity, don't do surgery on healthy teeth.
- Approve safe effective drugs, don't approve other drugs
- Punish the guilty, but not the innocent
- Block spam accounts, but don't block a regular user.

Unfortunately, we can't just snap our fingers and manifest our intentions into the world.
We have to come up with some process by which sort things into bins.
Medical testing with standardized thresholds, trials by jury, etc.

No matter how we set up the selection process, there will always be *some* 
false negatives (someone with cancer not getting surgery, a serial killer going lose) and 
false positives (someone without cancer getting surgery, an innocent person in jail).
And just saying "I don't need a selection process. I'll just go with my gut."? 
Well, that's a selection process too. 
In some cases, it's one that works quite well.
But your gut instinct will still often give you false results.

This is unavoidable. We can adjust our thresholds of evidence to [tilt the balance one way or another](https://en.wikipedia.org/wiki/Sensitivity_and_specificity). 
A dentist could choose to only perform surgery when the tooth is caved in and weeping pus, or they could choose to put fillings in every tooth that looks even a little bit suspicious (or heck, just pull all the teeth; no cavities possible that way). 
And we should always try to place the threshold in a way that balances the costs of each type of error.

Can we do better than a tradeoff? Can we reduce both types of error at the same time?
Yeah, if we have better information to start with.
For medical decisions, that means something like improved imaging technology.
For the court system, this means something like widespread surveilliance. 

The type 1 and type 2 errors are inescapable.


## INfo

- Hate speech is all alike. New ideas are all, by definition, novel.
- Punish criminals, improve safety, suppress social connections. Punish misinfo, supress info, supress info.

Main concern is that we like the sharing of info *because* it can spread non-standard new ideas.
Trying to stop the spread of misinformation by supressing anythign not from officail source
(say cdc for example)
will catch the correct new ideas as well as the bad.
Net negative effect if you believe, like I do, that the tail of positive new ideas can have outsized positive effects.



<!--
Lots of talk about the dangers of encryption.

My gut feeling is that anyone against encryption is a dangerous lunatic.
Encrypted communication is why the internet *works*.
It's the foundation of the modern electronic economy.
It's how I pay my bills and buy my groceries.
And we have a widespread non-encrypted communication system.
It's sms. It's dangerous insecure spam-ridden garbage.

Encryption in general is great. 
Only idiots disagree 
that I should be able to use encryption to send a message to my bank
asking them to transfer some of my money to another account.
But what about the specific use-case of using encryption 
to send a message to my friend
about whatever random thoughts pop into my head.
Shouldn't this dangerous activity be curtailed?!

Less anger:
There is actually a somewhat genuine debate to be had
about whether end to end encryption aplpications should have backdoors
built in for law enforcement.

The main arguments against are
- It would allow the government to punish those who oppose it.
- Any mistakes on the part of law enforcement would render everyone's information public

And the main argument for is:
- It would allow the government to punish those who oppose it.

And who knows? If your government is genuinely benevolant, it's possible that the spying apparatus will catch and punish the paedos and terrorists, and won't, say, arrest students for sharing messages critical of the national leader.

The question of whether the gov should violate privacy to catch criminals ultimately comes down to how virtuous your officials are.

But that question isn't why I'm typing this ramble. What got me irritated is the notion that privacy should be violated to prevent the spread of misinformation.
This is a bad idea. 
It isn't just a question of how virtuous the government is.
It's a fundamental misunderstanding of the very concept of information.

---

[Here's the post that irritated me.](https://www.nytimes.com/2021/02/03/technology/personaltech/telegram-signal-misinformation.html?smtyp=cur&smid=tw-nytimes)

> The apps offer “end to end encryption,” which is a jargony way to describe messages that get scrambled to become indecipherable to anyone except for the sender and the recipient.  
>The obvious benefit is that people are ensured privacy. The possible downside is that it’s tougher for the companies and law enforcement to hold misinformation spreaders and criminals accountable because their messages won’t be accessible.-->




