---
title: "EM for GMMs and Clustering"
excerpt_separator: "<!--more-->"
categories:
  - textbook
tags:
  - Machine Learning
  - Cluster Analysis
  - Moments
  - Econ 8791
---

"Machine Learning: A Probabilistic Perspective" by Kevin Murphy --- Chapter 11.4.2.5-11.4.2.7 and Chapter 25.

---

# 11.4.2.5-7 - EM for GMMs - K means

Book calls K-means a "popular variant of  EM algorithm for GMMs"  
"Hard EM"

1. Initialize $m_k$
2. repeat until converged:
   1. Assign each point to closest cluster center. $z_i=\argmin_k || x_i -\mu_k ||_2^2$
   2. Update each cluster center as mean of points in cluster. 
    
   $$\mu_k = \frac{1}{N_k}\sum_{i:z_i=k}x_i$$

"can be
interpreted as a greedy algorithm for approximately minimizing a loss function related to data
compression"

## Initialization

- Random
- **farthest point clustering** or **k-means++** : pick first at random, rest weighted towards those far away
- speech recognition likes to incrementally grow gmms. split up clusters and dissapear them if they don't behave


# 25 - Clustering

Flat clustering is $O(ND)$. Hierarchical is $O(N^2 \log N)$;

Hamming distance for categorical variables

## validation

> The validation of clustering structures is the most difficult and frustrating part of cluster
analysis. Without a strong effort in this direction, cluster analysis will remain a black art
accessible only to those true believers who have experience and great courage. â€” *Jain
and Dubes (Jain and Dubes 1988)*

If $k$ denotes cluster and $c$ denotes class, then **purity of a cluster** is biggest single-class portion of the cluster:

$$p_k \equiv \max_c \frac{N_{kc}}{N_k}$$

**Rand Index** is fraction of clustering decisions which are correct, compared to some reference. Adjusted Rand Index doesn't give credit for what random clustering could do.